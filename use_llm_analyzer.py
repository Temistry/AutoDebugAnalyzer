import os
import argparse
from llm_code_analyzer import LLMCodeAnalyzer
from typing import List, Dict, Any

# llm ê¸°ë°˜ ì½”ë“œ ë¶„ì„ê¸°
# ë²„ê·¸ ë¦¬í¬íŠ¸ ë¶„ì„ í›„ ì†ŒìŠ¤ì½”ë“œ ë¶„ì„ í›„ ë§¤ì¹­
# ì»¨í…ìŠ¤íŠ¸ ì§€ì‹ í™œìš©
# ê²Œì„ ìŠ¤í¬ë¦½íŠ¸ í™œìš©
# ì½”ë“œ ì²­í¬ ë¶„í• 
# ì°¸ì¡° ì½”ë“œ ì¶œë ¥
# ë¶„ì„ ì‹ ë¢°ë„ í‘œì‹œ
# ìˆ˜ì • ì œì•ˆ ìƒì„±

def load_bug_report(file_path: str) -> str:
    """ë²„ê·¸ ë¦¬í¬íŠ¸ íŒŒì¼ ë¡œë“œ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"[âŒ ì˜¤ë¥˜] ë²„ê·¸ ë¦¬í¬íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return ""

def load_source_files(directory: str, extensions=('.cpp', '.h')) -> List[Dict[str, Any]]:
    """ì†ŒìŠ¤ íŒŒì¼ ë¡œë“œ ë° ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    
    print(f"[ğŸ”] {directory} ì—ì„œ ì†ŒìŠ¤ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
    
    try:
        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith(extensions):
                    file_path = os.path.join(root, file)
                    
                    try:
                        # ANSI(CP949) ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ì‹œë„
                        with open(file_path, 'r', encoding='cp949', errors='replace') as f:
                            content = f.read()
                            
                        # ì²­í¬ë¡œ ë¶„í•  (100ì¤„ ë‹¨ìœ„)
                        lines = content.split('\n')
                        chunk_size = 100
                        
                        for i in range(0, len(lines), chunk_size):
                            end_idx = min(i + chunk_size, len(lines))
                            chunk_content = '\n'.join(lines[i:end_idx])
                            
                            if chunk_content.strip():  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ
                                chunks.append({
                                    'file_path': file_path,
                                    'start_line': i + 1,
                                    'end_line': end_idx,
                                    'content': chunk_content
                                })
                                
                    except UnicodeDecodeError:
                        # CP949 ì‹¤íŒ¨ ì‹œ EUC-KRë¡œ ì‹œë„
                        try:
                            with open(file_path, 'r', encoding='euc-kr', errors='replace') as f:
                                content = f.read()
                                
                            # ê°™ì€ ì²­í¬ ë¶„í•  ë¡œì§ ë°˜ë³µ
                            lines = content.split('\n')
                            for i in range(0, len(lines), chunk_size):
                                end_idx = min(i + chunk_size, len(lines))
                                chunk_content = '\n'.join(lines[i:end_idx])
                                
                                if chunk_content.strip():
                                    chunks.append({
                                        'file_path': file_path,
                                        'start_line': i + 1,
                                        'end_line': end_idx,
                                        'content': chunk_content
                                    })
                        except Exception as e2:
                            print(f"[âš ï¸] {file_path} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e2}")
                    
                    except Exception as e:
                        print(f"[âš ï¸] {file_path} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        
        print(f"[âœ…] {len(chunks)}ê°œ ì½”ë“œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return chunks
        
    except Exception as e:
        print(f"[âŒ] ì†ŒìŠ¤ íŒŒì¼ ìŠ¤ìº” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='LLM ê¸°ë°˜ ì½”ë“œ ë¶„ì„ ë„êµ¬')
    parser.add_argument('--bug_report', type=str, default="D:/data/GersangDebugAutomation/bug_report.txt",
                        help='ë²„ê·¸ ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--source_dir', type=str, default="C:/data/Branch_Trunk_bugfix",
                        help='ì†ŒìŠ¤ ì½”ë“œ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--knowledge', type=str, default="dev_knowledge.txt",
                        help='ê°œë°œì ì§€ì‹ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„ ì§„í–‰)')
    parser.add_argument('--api_url', type=str, default="http://192.168.102.166:1234",
                        help='LLM API ì„œë²„ ì£¼ì†Œ')
    parser.add_argument('--model', type=str, default="eeve-korean-instruct-10.8b-v1.0",
                        help='ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--script_dir', type=str, default="C:/data/GCS",
                        help='ê²Œì„ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ì„¤ì •
    bug_report_path = args.bug_report
    source_dir = args.source_dir
    knowledge_file = args.knowledge if os.path.exists(args.knowledge) else None
    script_dir = args.script_dir if os.path.exists(args.script_dir) else None
    
    if knowledge_file:
        print(f"[ğŸ“š] ê°œë°œì ì§€ì‹ íŒŒì¼: {knowledge_file}")
    else:
        print(f"[â„¹ï¸] ê°œë°œì ì§€ì‹ íŒŒì¼({args.knowledge})ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    if script_dir:
        print(f"[ğŸ“œ] ê²Œì„ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
    else:
        print(f"[â„¹ï¸] ê²Œì„ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬({args.script_dir})ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # 1. ë²„ê·¸ ë¦¬í¬íŠ¸ ë¡œë“œ
    print(f"\n[ğŸ“„] ë²„ê·¸ ë¦¬í¬íŠ¸ ë¡œë“œ ì¤‘: {bug_report_path}")
    bug_report = load_bug_report(bug_report_path)
    if not bug_report:
        print("[âŒ] ë²„ê·¸ ë¦¬í¬íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 2. LLM ë¶„ì„ê¸° ì´ˆê¸°í™”
    print("\n[ğŸ¤–] LLM ì½”ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
    analyzer = LLMCodeAnalyzer(
        api_url=args.api_url,
        model_name=args.model,
        knowledge_file=knowledge_file,
        script_dir=script_dir
    )
    
    # 3. ë²„ê·¸ ë¦¬í¬íŠ¸ ë¶„ì„
    print("\n[ğŸ”] ë²„ê·¸ ë¦¬í¬íŠ¸ ë¶„ì„ ì¤‘...")
    bug_analysis = analyzer.analyze_bug_report(bug_report)
    
    # 4. ì†ŒìŠ¤ ì½”ë“œ ë¡œë“œ
    print("\n[ğŸ“‚] ì†ŒìŠ¤ ì½”ë“œ ë¡œë“œ ì¤‘...")
    code_chunks = load_source_files(source_dir)
    if not code_chunks:
        print("[âŒ] ì†ŒìŠ¤ ì½”ë“œ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 5. ì½”ë“œ ë¬¸ë§¥ ë§¤ì¹­
    print("\n[ğŸ”„] ì½”ë“œ ë¬¸ë§¥ ë¶„ì„ ì¤‘...")
    matching_chunks = analyzer.match_with_code_context(bug_analysis, code_chunks, top_n=5)
    
    # 6. ê²°ê³¼ ì¶œë ¥
    print("\n[ğŸ“Š] === ë¶„ì„ ê²°ê³¼ ===")
    print(f"ë²„ê·¸ ìœ í˜•: {bug_analysis.get('bug_type', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
    print(f"ì‹¬ê°ë„: {bug_analysis.get('severity', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
    print(f"ë²„ê·¸ ìš”ì•½: {bug_analysis.get('summary', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
    print(f"ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(bug_analysis.get('keywords', ['ì—†ìŒ']))}")
    
    print("\n[ğŸ”] ì˜ì‹¬ ì½”ë“œ ì˜ì—­:")
    for i, chunk in enumerate(matching_chunks, 1):
        file_path = chunk['file_path']
        file_name = os.path.basename(file_path)
        
        print(f"\n{i}. íŒŒì¼: {file_name}")
        print(f"   ì „ì²´ ê²½ë¡œ: {file_path}")
        print(f"   ì½”ë“œ ìœ„ì¹˜: {chunk['start_line']}~{chunk['end_line']} ë¼ì¸")
        
        # ì˜ì‹¬ ë¼ì¸ ì •ë³´ ì¶œë ¥
        suspected_lines = chunk.get('suspected_lines', [])
        if suspected_lines:
            print(f"   ì˜ì‹¬ ë¼ì¸: {', '.join(map(str, suspected_lines))}")
            
            # ì°¸ì¡°ëœ ì½”ë“œ ì¶œë ¥ (ìƒˆë¡œìš´ í˜•ì‹)
            referenced_code = chunk.get('referenced_code', [])
            if referenced_code:
                print("\n   ì°¸ì¡°ëœ ì½”ë“œ:")
                for ref in referenced_code:
                    line_num = ref.get('line', '?')
                    code = ref.get('code', 'ì½”ë“œ ì •ë³´ ì—†ìŒ')
                    reason = ref.get('reason', '')
                    print(f"   {line_num}ë²ˆ ì¤„: {code}")
                    if reason:
                        print(f"      â†³ ì´ìœ : {reason}")
        
        # ë¶„ì„ ì‹ ë¢°ë„ í‘œì‹œ
        confidence = chunk.get('confidence', 'ì•Œ ìˆ˜ ì—†ìŒ')
        print(f"   ë¶„ì„ ì‹ ë¢°ë„: {confidence}")
        print(f"   ê´€ë ¨ì„± ì ìˆ˜: {chunk.get('relevance_score', 0)}/10")
        print(f"   ë¶„ì„: {chunk.get('reasoning', 'ì •ë³´ ì—†ìŒ')}")
    
    # 7. ìµœìƒìœ„ ë§¤ì¹­ì— ëŒ€í•œ ìˆ˜ì • ì œì•ˆ ìƒì„±
    if matching_chunks:
        print("\n[ğŸ’¡] ìˆ˜ì • ì œì•ˆ ìƒì„± ì¤‘...")
        fix_suggestion = analyzer.generate_fix_suggestion(bug_report, matching_chunks[0])
        print("\n[ğŸ› ï¸] ìˆ˜ì • ì œì•ˆ:")
        print(fix_suggestion)
    
    print("\n[âœ…] ë¶„ì„ ì™„ë£Œ")

if __name__ == "__main__":
    main() 